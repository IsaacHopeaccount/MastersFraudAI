{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m49708/49708\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 979us/step\n",
      "\n",
      "Epoch 1 - Best Threshold: 0.9979\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00   1588602\n",
      "           1       0.84      0.62      0.71      2053\n",
      "\n",
      "    accuracy                           1.00   1590655\n",
      "   macro avg       0.92      0.81      0.86   1590655\n",
      "weighted avg       1.00      1.00      1.00   1590655\n",
      "\n",
      "\u001b[1m208504/208504\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1064s\u001b[0m 5ms/step - accuracy: 0.9522 - loss: 0.1148 - val_accuracy: 0.9678 - val_loss: 0.0645\n",
      "Epoch 2/50\n",
      "\u001b[1m49708/49708\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 731us/step\n",
      "\n",
      "Epoch 2 - Best Threshold: 0.9956\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00   1588602\n",
      "           1       0.88      0.66      0.76      2053\n",
      "\n",
      "    accuracy                           1.00   1590655\n",
      "   macro avg       0.94      0.83      0.88   1590655\n",
      "weighted avg       1.00      1.00      1.00   1590655\n",
      "\n",
      "\u001b[1m208504/208504\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m427s\u001b[0m 2ms/step - accuracy: 0.9642 - loss: 0.0891 - val_accuracy: 0.9746 - val_loss: 0.0550\n",
      "Epoch 3/50\n",
      "\u001b[1m49708/49708\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 751us/step\n",
      "\n",
      "Epoch 3 - Best Threshold: 0.9963\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00   1588602\n",
      "           1       0.87      0.68      0.76      2053\n",
      "\n",
      "    accuracy                           1.00   1590655\n",
      "   macro avg       0.94      0.84      0.88   1590655\n",
      "weighted avg       1.00      1.00      1.00   1590655\n",
      "\n",
      "\u001b[1m208504/208504\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m394s\u001b[0m 2ms/step - accuracy: 0.9653 - loss: 0.0870 - val_accuracy: 0.9710 - val_loss: 0.0551\n",
      "Epoch 4/50\n",
      "\u001b[1m49708/49708\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 729us/step\n",
      "\n",
      "Epoch 4 - Best Threshold: 0.9965\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00   1588602\n",
      "           1       0.90      0.68      0.77      2053\n",
      "\n",
      "    accuracy                           1.00   1590655\n",
      "   macro avg       0.95      0.84      0.89   1590655\n",
      "weighted avg       1.00      1.00      1.00   1590655\n",
      "\n",
      "\u001b[1m208504/208504\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m394s\u001b[0m 2ms/step - accuracy: 0.9666 - loss: 0.0843 - val_accuracy: 0.9660 - val_loss: 0.0620\n",
      "Epoch 5/50\n",
      "\u001b[1m49708/49708\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 690us/step\n",
      "\n",
      "Epoch 5 - Best Threshold: 0.9942\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00   1588602\n",
      "           1       0.87      0.66      0.75      2053\n",
      "\n",
      "    accuracy                           1.00   1590655\n",
      "   macro avg       0.93      0.83      0.88   1590655\n",
      "weighted avg       1.00      1.00      1.00   1590655\n",
      "\n",
      "\u001b[1m208504/208504\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m392s\u001b[0m 2ms/step - accuracy: 0.9672 - loss: 0.0842 - val_accuracy: 0.9687 - val_loss: 0.0588\n",
      "Epoch 6/50\n",
      "\u001b[1m49708/49708\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 800us/step\n",
      "\n",
      "Epoch 6 - Best Threshold: 0.9863\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00   1588602\n",
      "           1       0.84      0.69      0.76      2053\n",
      "\n",
      "    accuracy                           1.00   1590655\n",
      "   macro avg       0.92      0.85      0.88   1590655\n",
      "weighted avg       1.00      1.00      1.00   1590655\n",
      "\n",
      "\u001b[1m208504/208504\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m418s\u001b[0m 2ms/step - accuracy: 0.9674 - loss: 0.0845 - val_accuracy: 0.9654 - val_loss: 0.0596\n",
      "Epoch 7/50\n",
      "\u001b[1m49708/49708\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 740us/step\n",
      "\n",
      "Epoch 7 - Best Threshold: 0.9958\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00   1588602\n",
      "           1       0.92      0.71      0.80      2053\n",
      "\n",
      "    accuracy                           1.00   1590655\n",
      "   macro avg       0.96      0.85      0.90   1590655\n",
      "weighted avg       1.00      1.00      1.00   1590655\n",
      "\n",
      "\u001b[1m208504/208504\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m413s\u001b[0m 2ms/step - accuracy: 0.9673 - loss: 0.0831 - val_accuracy: 0.9644 - val_loss: 0.0606\n",
      "Training fold 1...\n",
      "Unique classes in y_tr: [0 1]\n",
      "\u001b[1m41701/41701\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 650us/step\n",
      "\u001b[1m49708/49708\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 643us/step\n",
      "Training fold 2...\n",
      "Unique classes in y_tr: [0 1]\n",
      "\u001b[1m41701/41701\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 570us/step\n",
      "\u001b[1m49708/49708\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 589us/step\n",
      "Training fold 3...\n",
      "Unique classes in y_tr: [0 1]\n",
      "\u001b[1m41701/41701\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 553us/step\n",
      "\u001b[1m49708/49708\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 545us/step\n",
      "Training fold 4...\n",
      "Unique classes in y_tr: [0 1]\n",
      "\u001b[1m41701/41701\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 561us/step\n",
      "\u001b[1m49708/49708\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 575us/step\n",
      "Training fold 5...\n",
      "Unique classes in y_tr: [0 1]\n",
      "\u001b[1m41701/41701\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 546us/step\n",
      "\u001b[1m49708/49708\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 547us/step\n",
      "\n",
      "Ensemble Model - Best Threshold: 0.9974\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00   1588602\n",
      "           1       0.94      0.89      0.92      2053\n",
      "\n",
      "    accuracy                           1.00   1590655\n",
      "   macro avg       0.97      0.95      0.96   1590655\n",
      "weighted avg       1.00      1.00      1.00   1590655\n",
      "\n",
      "Average Precision-Recall Score: 0.9666\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import classification_report, precision_recall_curve, average_precision_score\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "# Load and preprocess the dataset\n",
    "dataset3 = pd.read_csv('dataset3- Paysim/PS_20174392719_1491204439457_log.csv')\n",
    "dataset3.drop_duplicates(inplace=True)\n",
    "\n",
    "# Ensure numerical columns are of numeric type\n",
    "numeric_columns = ['amount', 'oldbalanceOrg', 'newbalanceOrig', 'oldbalanceDest', 'newbalanceDest']\n",
    "for col in numeric_columns:\n",
    "    dataset3[col] = pd.to_numeric(dataset3[col], errors='coerce')\n",
    "\n",
    "# Drop rows where 'isFraud' is NaN\n",
    "dataset3 = dataset3.dropna(subset=['isFraud'])\n",
    "\n",
    "# Convert categorical column 'type' into dummy/indicator variables\n",
    "if 'type' in dataset3.columns:\n",
    "    dataset3 = pd.get_dummies(dataset3, columns=['type'], drop_first=True)\n",
    "\n",
    "# Create new features\n",
    "dataset3['balanceDiff'] = dataset3['oldbalanceOrg'] - dataset3['newbalanceOrig']\n",
    "dataset3['balanceRatio'] = dataset3['newbalanceOrig'] / (dataset3['oldbalanceOrg'] + 1e-9)\n",
    "dataset3['avgAmountOrig'] = dataset3.groupby('nameOrig')['amount'].transform('mean')\n",
    "dataset3['avgAmountDest'] = dataset3.groupby('nameDest')['amount'].transform('mean')\n",
    "\n",
    "# Fill any remaining NaN values\n",
    "dataset3.fillna(0, inplace=True)\n",
    "\n",
    "# Define features (X) and target (y)\n",
    "X = dataset3.drop(columns=['isFraud', 'nameOrig', 'nameDest'])\n",
    "y = dataset3['isFraud'].astype(int)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Scale numeric features\n",
    "scaler = StandardScaler()\n",
    "X_train_full_scaled = scaler.fit_transform(X_train_full)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Apply SMOTE to handle class imbalance on training data\n",
    "smote = SMOTE(sampling_strategy=0.4, random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train_full_scaled, y_train_full)\n",
    "\n",
    "# Ensure y_train_resampled is a numpy array of int type\n",
    "y_train_resampled = np.array(y_train_resampled).astype(int)\n",
    "\n",
    "# Reshape data for CNN\n",
    "new_shape = (3, 5, 1)  # Adjust based on the number of features\n",
    "X_train_resampled_cnn = X_train_resampled.reshape(-1, *new_shape)\n",
    "X_test_cnn = X_test_scaled.reshape(-1, *new_shape)\n",
    "\n",
    "# Compute class weights for the full resampled training data\n",
    "class_weights_values = class_weight.compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(y_train_resampled),\n",
    "    y=y_train_resampled\n",
    ")\n",
    "class_weights = dict(zip(np.unique(y_train_resampled), class_weights_values))\n",
    "\n",
    "# Define the custom callback for dynamic thresholding\n",
    "class DynamicThresholdCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, X_val, y_val):\n",
    "        super().__init__()\n",
    "        self.X_val = X_val\n",
    "        self.y_val = y_val\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        y_proba = self.model.predict(self.X_val).flatten()\n",
    "        precision, recall, thresholds = precision_recall_curve(self.y_val, y_proba)\n",
    "        f1_scores = 2 * precision * recall / (precision + recall + 1e-9)\n",
    "        best_idx = np.argmax(f1_scores)\n",
    "        best_threshold = thresholds[best_idx] if best_idx < len(thresholds) else 0.5\n",
    "        y_pred = (y_proba >= best_threshold).astype(int)\n",
    "        print(f\"\\nEpoch {epoch + 1} - Best Threshold: {best_threshold:.4f}\")\n",
    "        print(classification_report(self.y_val, y_pred))\n",
    "\n",
    "# Implement early stopping\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=5,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "# Define CNN model with Input layer to fix the UserWarning\n",
    "model_cnn = Sequential([\n",
    "    Input(shape=new_shape),\n",
    "    Conv2D(32, (2, 2), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Dropout(0.25),\n",
    "    Flatten(),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model with class weights\n",
    "model_cnn.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train CNN model with the custom callback and early stopping\n",
    "model_cnn.fit(\n",
    "    X_train_resampled_cnn, y_train_resampled,\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_test_cnn, y_test),\n",
    "    class_weight=class_weights,\n",
    "    callbacks=[DynamicThresholdCallback(X_test_cnn, y_test), early_stopping]\n",
    ")\n",
    "\n",
    "# Train XGBoost model with adjusted scale_pos_weight\n",
    "scale_pos_weight = class_weights[0] / class_weights[1]\n",
    "\n",
    "clf_xgb = XGBClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=10,\n",
    "    learning_rate=0.2,\n",
    "    subsample=0.75,\n",
    "    colsample_bytree=0.75,\n",
    "    random_state=42,\n",
    "    scale_pos_weight=scale_pos_weight  # Adjust scale_pos_weight\n",
    ")\n",
    "clf_xgb.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Generate meta-features using cross-validation\n",
    "meta_train = np.zeros((X_train_resampled.shape[0], 2))\n",
    "meta_test = np.zeros((X_test_scaled.shape[0], 2))\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(X_train_resampled, y_train_resampled)):\n",
    "    print(f\"Training fold {fold + 1}...\")\n",
    "    X_tr, X_val = X_train_resampled[train_idx], X_train_resampled[val_idx]\n",
    "    y_tr, y_val = y_train_resampled[train_idx], y_train_resampled[val_idx]\n",
    "\n",
    "    # Ensure y_tr and y_val are numpy arrays of int type\n",
    "    y_tr = np.array(y_tr).astype(int)\n",
    "    y_val = np.array(y_val).astype(int)\n",
    "\n",
    "    # Check unique classes in y_tr\n",
    "    unique_classes = np.unique(y_tr)\n",
    "    print(f\"Unique classes in y_tr: {unique_classes}\")\n",
    "\n",
    "    if len(unique_classes) < 2:\n",
    "        print(f\"Only one class present in y_tr for fold {fold + 1}. Skipping this fold.\")\n",
    "        continue\n",
    "\n",
    "    # Compute class weights for the current fold\n",
    "    class_weights_fold_values = class_weight.compute_class_weight(\n",
    "        class_weight='balanced',\n",
    "        classes=unique_classes,\n",
    "        y=y_tr\n",
    "    )\n",
    "    class_weights_fold = dict(zip(unique_classes, class_weights_fold_values))\n",
    "\n",
    "    # Adjust scale_pos_weight for XGBoost\n",
    "    scale_pos_weight_fold = class_weights_fold[0] / class_weights_fold[1]\n",
    "\n",
    "    # Reshape data for CNN\n",
    "    X_tr_cnn = X_tr.reshape(-1, *new_shape)\n",
    "    X_val_cnn = X_val.reshape(-1, *new_shape)\n",
    "\n",
    "    # Train base models\n",
    "    # CNN model\n",
    "    model_cnn_fold = tf.keras.models.clone_model(model_cnn)\n",
    "    model_cnn_fold.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    model_cnn_fold.fit(\n",
    "        X_tr_cnn, y_tr,\n",
    "        epochs=10,\n",
    "        batch_size=32,\n",
    "        class_weight=class_weights_fold,\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "    # XGBoost model\n",
    "    clf_xgb_fold = XGBClassifier(\n",
    "        n_estimators=100,\n",
    "        max_depth=10,\n",
    "        learning_rate=0.2,\n",
    "        subsample=0.75,\n",
    "        colsample_bytree=0.75,\n",
    "        random_state=42,\n",
    "        scale_pos_weight=scale_pos_weight_fold\n",
    "    )\n",
    "    clf_xgb_fold.fit(X_tr, y_tr)\n",
    "\n",
    "    # Predict on validation fold\n",
    "    cnn_val_preds = model_cnn_fold.predict(X_val_cnn).flatten()\n",
    "    xgb_val_preds = clf_xgb_fold.predict_proba(X_val)[:, 1]\n",
    "\n",
    "    # Store predictions as meta-features\n",
    "    meta_train[val_idx, 0] = cnn_val_preds\n",
    "    meta_train[val_idx, 1] = xgb_val_preds\n",
    "\n",
    "    # Predict on test set and average\n",
    "    cnn_test_preds = model_cnn_fold.predict(X_test_cnn).flatten()\n",
    "    xgb_test_preds = clf_xgb_fold.predict_proba(X_test_scaled)[:, 1]\n",
    "    meta_test[:, 0] += cnn_test_preds / skf.n_splits\n",
    "    meta_test[:, 1] += xgb_test_preds / skf.n_splits\n",
    "\n",
    "# Ensure meta_train and meta_test are properly populated\n",
    "if np.any(np.isnan(meta_train)) or np.any(np.isnan(meta_test)):\n",
    "    print(\"NaN values found in meta features. Please check the cross-validation loop.\")\n",
    "else:\n",
    "    # Train meta-model\n",
    "    meta_model = LogisticRegression(max_iter=10000, random_state=42)\n",
    "    meta_model.fit(meta_train, y_train_resampled)\n",
    "\n",
    "    # Evaluate the ensemble model\n",
    "    ensemble_proba = meta_model.predict_proba(meta_test)[:, 1]\n",
    "\n",
    "    # Find optimal threshold for ensemble model\n",
    "    precision, recall, thresholds = precision_recall_curve(y_test, ensemble_proba)\n",
    "    f1_scores = 2 * precision * recall / (precision + recall + 1e-9)\n",
    "    best_idx = np.argmax(f1_scores)\n",
    "    best_threshold = thresholds[best_idx] if best_idx < len(thresholds) else 0.5\n",
    "    ensemble_preds = (ensemble_proba >= best_threshold).astype(int)\n",
    "\n",
    "    print(f\"\\nEnsemble Model - Best Threshold: {best_threshold:.4f}\")\n",
    "    print(classification_report(y_test, ensemble_preds))\n",
    "\n",
    "    # Compute Average Precision Score\n",
    "    average_precision = average_precision_score(y_test, ensemble_proba)\n",
    "    print(f'Average Precision-Recall Score: {average_precision:.4f}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
